{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "testZalandoTiny0110.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwmlm7iBht2B",
        "colab_type": "code",
        "outputId": "ae2decce-5f12-4e15-93f2-59319ac0c154",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, GlobalAveragePooling2D, Flatten,Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.optimizers import RMSprop\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 50\n",
        "\n",
        "img_rows, img_cols = 28, 28\n",
        "nb_train_samples = 60000\n",
        "nb_validation_samples = 10000\n",
        "\n",
        "################## Def dataset ###########################\n",
        "\n",
        "#the data, split between train and test sets\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "#x_train = x_train.reshape(60000, 56)\n",
        "#x_test = x_test.reshape(10000, 56)\n",
        "\n",
        "if K.image_data_format() == 'channels first':\n",
        "  x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "  x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "  input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "  x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "  x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "  input_shape = (img_rows, img_cols, 1)\n",
        "  \n",
        "  \n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "#to ensure a good dynamic, help the learnig convergence\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "#convert class vectors to binary class matrices\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "################## Data augmentation ###################\n",
        "\n",
        "train_datagen = ImageDataGenerator(horizontal_flip=True)\n",
        "\n",
        "train_generator = train_datagen.flow(\n",
        "  x_train,\n",
        "  y_train,\n",
        "  batch_size=batch_size)\n",
        "\n",
        "################## Def model ###########################\n",
        "\n",
        "model = Sequential()\n",
        "\"\"\"\n",
        "model.add(Conv2D(16, kernel_size=(3,3),\n",
        "                activation='relu',\n",
        "                input_shape=input_shape))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(32, kernel_size=(3,3),\n",
        "                activation='relu',\n",
        "                input_shape=input_shape))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(16, kernel_size=(1,1),\n",
        "                activation='relu',\n",
        "                input_shape=input_shape))\n",
        "model.add(Conv2D(128, kernel_size=(3,3),\n",
        "                activation='relu',\n",
        "                input_shape=input_shape))\n",
        "model.add(Conv2D(16, kernel_size=(1,1),\n",
        "                activation='relu',\n",
        "                input_shape=input_shape))\n",
        "model.add(Conv2D(128, kernel_size=(3,3),\n",
        "                activation='relu',\n",
        "                input_shape=input_shape))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\"\"\"\n",
        "model.add(Conv2D(32, kernel_size=(1,1),\n",
        "                activation='relu',\n",
        "                input_shape=input_shape))\n",
        "model.add(Conv2D(256, kernel_size=(3,3),\n",
        "                activation='relu',\n",
        "                input_shape=input_shape))\n",
        "model.add(Conv2D(32, kernel_size=(1,1),\n",
        "                activation='relu',\n",
        "                input_shape=input_shape))\n",
        "model.add(Conv2D(256, kernel_size=(3,3),\n",
        "                activation='relu',\n",
        "                input_shape=input_shape))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(64, kernel_size=(1,1),\n",
        "                activation='relu',\n",
        "                input_shape=input_shape))\n",
        "model.add(Conv2D(512, kernel_size=(3,3),\n",
        "                activation='relu',\n",
        "                input_shape=input_shape))\n",
        "model.add(Conv2D(64, kernel_size=(1,1),\n",
        "                activation='relu',\n",
        "                input_shape=input_shape))\n",
        "model.add(Conv2D(512, kernel_size=(3,3),\n",
        "                activation='relu',\n",
        "                input_shape=input_shape))\n",
        "model.add(Conv2D(128, kernel_size=(1,1),\n",
        "                activation='relu',\n",
        "                input_shape=input_shape))\n",
        "model.add(Conv2D(1000, kernel_size=(1,1),\n",
        "                activation='relu',\n",
        "                input_shape=input_shape))\n",
        "model.add(GlobalAveragePooling2D())\n",
        "\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "#model.add(BatchNormalization())\n",
        "  \n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer = RMSprop(), metrics=['accuracy'])\n",
        "\n",
        "\n",
        "################## Learning ###########################\n",
        "\n",
        "#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
        "\n",
        "\n",
        "# checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\"\"\"\n",
        "earlystopping = EarlyStopping(monitor='val_acc',\n",
        "                              min_delta=0,\n",
        "                              patience=5,\n",
        "                              verbose=1, mode='auto')\n",
        "callbacks_list = [checkpoint, earlystopping]\n",
        "\"\"\"\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "#fit permet de lancer l'apprentissage, avec les donnees, les labels, la taille du batch, le nb d'epoque, la verbosite, et les donnees de validation d'epoque a epoque\n",
        "\"\"\"\n",
        "history = model.fit(x_train, y_train, \n",
        "                     batch_size=batch_size, \n",
        "                     epochs=epochs, \n",
        "                     verbose=1, \n",
        "                     validation_data=(x_test, y_test),\n",
        "                     callbacks=callbacks_list)\n",
        "\"\"\"\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=nb_train_samples // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=callbacks_list)\n",
        "\n",
        "#evaluate toward test dataset\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss: ', score[0])\n",
        "print('Test accuracy: ', score[1])\n",
        "\n",
        "model.save_weights('fashion_try_0110.h5')\n",
        "model.save('Test_FASHION_MNIST_V1.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_74 (Conv2D)           (None, 28, 28, 32)        64        \n",
            "_________________________________________________________________\n",
            "conv2d_75 (Conv2D)           (None, 26, 26, 256)       73984     \n",
            "_________________________________________________________________\n",
            "conv2d_76 (Conv2D)           (None, 26, 26, 32)        8224      \n",
            "_________________________________________________________________\n",
            "conv2d_77 (Conv2D)           (None, 24, 24, 256)       73984     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_29 (MaxPooling (None, 12, 12, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_78 (Conv2D)           (None, 12, 12, 64)        16448     \n",
            "_________________________________________________________________\n",
            "conv2d_79 (Conv2D)           (None, 10, 10, 512)       295424    \n",
            "_________________________________________________________________\n",
            "conv2d_80 (Conv2D)           (None, 10, 10, 64)        32832     \n",
            "_________________________________________________________________\n",
            "conv2d_81 (Conv2D)           (None, 8, 8, 512)         295424    \n",
            "_________________________________________________________________\n",
            "conv2d_82 (Conv2D)           (None, 8, 8, 128)         65664     \n",
            "_________________________________________________________________\n",
            "conv2d_83 (Conv2D)           (None, 8, 8, 1000)        129000    \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                10010     \n",
            "=================================================================\n",
            "Total params: 1,001,058\n",
            "Trainable params: 1,001,058\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "468/468 [==============================] - 35s 75ms/step - loss: 1.2287 - acc: 0.5375 - val_loss: 0.6307 - val_acc: 0.7743\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.77430, saving model to weights-improvement-01-0.77.hdf5\n",
            "Epoch 2/50\n",
            "468/468 [==============================] - 24s 52ms/step - loss: 0.5508 - acc: 0.8008 - val_loss: 0.4581 - val_acc: 0.8319\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.77430 to 0.83190, saving model to weights-improvement-02-0.83.hdf5\n",
            "Epoch 3/50\n",
            "468/468 [==============================] - 25s 53ms/step - loss: 0.4210 - acc: 0.8465 - val_loss: 0.4624 - val_acc: 0.8378\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.83190 to 0.83780, saving model to weights-improvement-03-0.84.hdf5\n",
            "Epoch 4/50\n",
            "468/468 [==============================] - 26s 55ms/step - loss: 0.3561 - acc: 0.8718 - val_loss: 0.3371 - val_acc: 0.8813\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.83780 to 0.88130, saving model to weights-improvement-04-0.88.hdf5\n",
            "Epoch 5/50\n",
            "468/468 [==============================] - 26s 56ms/step - loss: 0.3173 - acc: 0.8856 - val_loss: 0.3829 - val_acc: 0.8610\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.88130\n",
            "Epoch 6/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.2906 - acc: 0.8949 - val_loss: 0.2930 - val_acc: 0.8958\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.88130 to 0.89580, saving model to weights-improvement-06-0.90.hdf5\n",
            "Epoch 7/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.2729 - acc: 0.9017 - val_loss: 0.2724 - val_acc: 0.8993\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.89580 to 0.89930, saving model to weights-improvement-07-0.90.hdf5\n",
            "Epoch 8/50\n",
            "468/468 [==============================] - 26s 55ms/step - loss: 0.2495 - acc: 0.9104 - val_loss: 0.2801 - val_acc: 0.9003\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.89930 to 0.90030, saving model to weights-improvement-08-0.90.hdf5\n",
            "Epoch 9/50\n",
            "468/468 [==============================] - 26s 55ms/step - loss: 0.2387 - acc: 0.9142 - val_loss: 0.3353 - val_acc: 0.8907\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.90030\n",
            "Epoch 10/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.2242 - acc: 0.9190 - val_loss: 0.2617 - val_acc: 0.9136\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.90030 to 0.91360, saving model to weights-improvement-10-0.91.hdf5\n",
            "Epoch 11/50\n",
            "468/468 [==============================] - 26s 54ms/step - loss: 0.2131 - acc: 0.9222 - val_loss: 0.2657 - val_acc: 0.9088\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.91360\n",
            "Epoch 12/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.2013 - acc: 0.9273 - val_loss: 0.2388 - val_acc: 0.9165\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.91360 to 0.91650, saving model to weights-improvement-12-0.92.hdf5\n",
            "Epoch 13/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.1906 - acc: 0.9308 - val_loss: 0.2381 - val_acc: 0.9178\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.91650 to 0.91780, saving model to weights-improvement-13-0.92.hdf5\n",
            "Epoch 14/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.1817 - acc: 0.9341 - val_loss: 0.2560 - val_acc: 0.9112\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.91780\n",
            "Epoch 15/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.1718 - acc: 0.9375 - val_loss: 0.2423 - val_acc: 0.9187\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.91780 to 0.91870, saving model to weights-improvement-15-0.92.hdf5\n",
            "Epoch 16/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.1666 - acc: 0.9398 - val_loss: 0.2332 - val_acc: 0.9212\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.91870 to 0.92120, saving model to weights-improvement-16-0.92.hdf5\n",
            "Epoch 17/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.1573 - acc: 0.9432 - val_loss: 0.2483 - val_acc: 0.9240\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.92120 to 0.92400, saving model to weights-improvement-17-0.92.hdf5\n",
            "Epoch 18/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.1458 - acc: 0.9472 - val_loss: 0.2664 - val_acc: 0.9179\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.92400\n",
            "Epoch 19/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.1413 - acc: 0.9487 - val_loss: 0.2856 - val_acc: 0.9067\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.92400\n",
            "Epoch 20/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.1332 - acc: 0.9515 - val_loss: 0.2824 - val_acc: 0.9119\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.92400\n",
            "Epoch 21/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.1275 - acc: 0.9534 - val_loss: 0.2511 - val_acc: 0.9175\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.92400\n",
            "Epoch 22/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.1208 - acc: 0.9561 - val_loss: 0.3454 - val_acc: 0.9127\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.92400\n",
            "Epoch 23/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.1142 - acc: 0.9582 - val_loss: 0.3249 - val_acc: 0.9118\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.92400\n",
            "Epoch 24/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.1087 - acc: 0.9615 - val_loss: 0.2658 - val_acc: 0.9243\n",
            "\n",
            "Epoch 00024: val_acc improved from 0.92400 to 0.92430, saving model to weights-improvement-24-0.92.hdf5\n",
            "Epoch 25/50\n",
            "468/468 [==============================] - 26s 55ms/step - loss: 0.1031 - acc: 0.9621 - val_loss: 0.2744 - val_acc: 0.9204\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.92430\n",
            "Epoch 26/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.0981 - acc: 0.9653 - val_loss: 0.2765 - val_acc: 0.9222\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.92430\n",
            "Epoch 27/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.0926 - acc: 0.9671 - val_loss: 0.3131 - val_acc: 0.9222\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.92430\n",
            "Epoch 28/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.0874 - acc: 0.9685 - val_loss: 0.3173 - val_acc: 0.9177\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.92430\n",
            "Epoch 29/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.0856 - acc: 0.9707 - val_loss: 0.3436 - val_acc: 0.9197\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.92430\n",
            "Epoch 30/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.0801 - acc: 0.9711 - val_loss: 0.3336 - val_acc: 0.9214\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.92430\n",
            "Epoch 31/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.0800 - acc: 0.9715 - val_loss: 0.3270 - val_acc: 0.9188\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.92430\n",
            "Epoch 32/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.0732 - acc: 0.9739 - val_loss: 0.3531 - val_acc: 0.9212\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.92430\n",
            "Epoch 33/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.0710 - acc: 0.9755 - val_loss: 0.3383 - val_acc: 0.9218\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.92430\n",
            "Epoch 34/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.0678 - acc: 0.9763 - val_loss: 0.3771 - val_acc: 0.9123\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.92430\n",
            "Epoch 35/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.0665 - acc: 0.9766 - val_loss: 0.3551 - val_acc: 0.9160\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.92430\n",
            "Epoch 36/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.0623 - acc: 0.9775 - val_loss: 0.4173 - val_acc: 0.9199\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.92430\n",
            "Epoch 37/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.0601 - acc: 0.9795 - val_loss: 0.3819 - val_acc: 0.9214\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.92430\n",
            "Epoch 38/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.0620 - acc: 0.9782 - val_loss: 0.4761 - val_acc: 0.9049\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.92430\n",
            "Epoch 39/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.0588 - acc: 0.9795 - val_loss: 0.4025 - val_acc: 0.9194\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.92430\n",
            "Epoch 40/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.0580 - acc: 0.9803 - val_loss: 0.4482 - val_acc: 0.9221\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.92430\n",
            "Epoch 41/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.0525 - acc: 0.9814 - val_loss: 0.4305 - val_acc: 0.9154\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.92430\n",
            "Epoch 42/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.0520 - acc: 0.9822 - val_loss: 0.4191 - val_acc: 0.9161\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.92430\n",
            "Epoch 43/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.0543 - acc: 0.9814 - val_loss: 0.4201 - val_acc: 0.9169\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.92430\n",
            "Epoch 44/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.0497 - acc: 0.9830 - val_loss: 0.4162 - val_acc: 0.9161\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.92430\n",
            "Epoch 45/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.0503 - acc: 0.9826 - val_loss: 0.4069 - val_acc: 0.9219\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.92430\n",
            "Epoch 46/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.0514 - acc: 0.9827 - val_loss: 0.4543 - val_acc: 0.9179\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.92430\n",
            "Epoch 47/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.0476 - acc: 0.9839 - val_loss: 0.4519 - val_acc: 0.9211\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.92430\n",
            "Epoch 48/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.0498 - acc: 0.9839 - val_loss: 0.4534 - val_acc: 0.9212\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.92430\n",
            "Epoch 49/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.0483 - acc: 0.9839 - val_loss: 0.4338 - val_acc: 0.9160\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.92430\n",
            "Epoch 50/50\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.0447 - acc: 0.9855 - val_loss: 0.4886 - val_acc: 0.9197\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.92430\n",
            "Test loss:  0.48864701474339234\n",
            "Test accuracy:  0.9197\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx6sDiAoWWcg",
        "colab_type": "text"
      },
      "source": [
        "model from : https://www.dlology.com/blog/one-simple-trick-to-train-keras-model-faster-with-batch-normalization/\n",
        "\n",
        "Epoch 00050: val_acc did not improve from 0.90950\n",
        "\n",
        "Test loss:  0.7327184855340101\n",
        "\n",
        "Test accuracy:  0.902\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#En rajoutant Dense 128\n",
        "\n",
        "Epoch 00050: val_acc did not improve from 0.91210# \n",
        "\n",
        "Test loss:  0.7428650218252558\n",
        "\n",
        "Test accuracy:  0.9072\n",
        "\n",
        "\n",
        "\n",
        "#En rajoutant Dense 256\n",
        "\n",
        "Total params: 246,008\n",
        "\n",
        "Epoch 00050: val_acc did not improvefrom 0.91430\n",
        "\n",
        "Test loss:  0.6275242215685546\n",
        "\n",
        "Test accuracy:  0.9134\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Dernier résultat :\n",
        "\n",
        "Epoch 00050: val_acc did not improve from 0.91700\n",
        "\n",
        "Test loss:  0.7110278423965914\n",
        "\n",
        "Test accuracy:  0.9069"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6Cw0p8qq2pt",
        "colab_type": "text"
      },
      "source": [
        "#Data Augmentation\n",
        "En ajoutant de la data augmentation via un horizontal_flip : \n",
        "\n",
        "Total params : 525,048\n",
        "\n",
        "Test loss:  0.3304\n",
        "\n",
        "Test accuracy:  0.9150"
      ]
    }
  ]
}