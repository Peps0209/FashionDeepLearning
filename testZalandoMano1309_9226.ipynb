{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "testZalandoMano1309_9226.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lj8ApOWFsTQp",
        "colab_type": "code",
        "outputId": "b11db5d7-ea71-40f9-ee24-130603b79f4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten,Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.optimizers import RMSprop\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 50\n",
        "\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "################## Def dataset ###########################\n",
        "\n",
        "#the data, split between train and test sets\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "#x_train = x_train.reshape(60000, 56)\n",
        "#x_test = x_test.reshape(10000, 56)\n",
        "\n",
        "if K.image_data_format() == 'channels first':\n",
        "  x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "  x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "  input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "  x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "  x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "  input_shape = (img_rows, img_cols, 1)\n",
        "  \n",
        "  \n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "#to ensure a good dynamic, help the learnig convergence\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "#convert class vectors to binary class matrices\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "################## Data augmentation ###################\n",
        "\n",
        "################## Def model ###########################\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, kernel_size=(3,3),\n",
        "                activation='relu',\n",
        "                input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Dropout(0,25))\n",
        "model.add(Conv2D(32, (5,5), activation='relu')) # meilleur sans les deux couches suivantes\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(64, (3,3), activation='relu')) ##\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2))) ##\n",
        "\n",
        "#Epoch 00050: val_acc did not improve from 0.91760\n",
        "#Test loss:  0.6874618720195256\n",
        "#Test accuracy:  0.9066\n",
        "\n",
        "#Avec batchNormalization\n",
        "#Epoch 00050: val_acc did not improve from 0.91740\n",
        "#Test loss:  0.6105943850596436\n",
        "#Test accuracy:  0.9144\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0,5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "#model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "#            optimizer=keras.optimizers.Adadelta(),\n",
        "#             metrics=['accuracy'])\n",
        "#Epoch 00050: val_acc did not improve from 0.91970\n",
        "#Test loss:  0.6960430979938712\n",
        "#Test accuracy:  0.917\n",
        "  \n",
        "  \n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer = RMSprop(), metrics=['accuracy'])\n",
        "##Epoch 00050: val_acc did not improve from 0.92170\n",
        "##Test loss:  0.7203312129669707\n",
        "##Test accuracy:  0.9142\n",
        "\n",
        "################## Learning ###########################\n",
        "\n",
        "#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
        "\n",
        "\n",
        "# checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "#fit permet de lancer l'apprentissage, avec les donnees, les labels, la taille du batch, le nb d'epoque, la verbosite, et les donnees de validation d'epoque a epoque\n",
        "history = model.fit(x_train, y_train, \n",
        "                     batch_size=batch_size, \n",
        "                     epochs=epochs, \n",
        "                     verbose=1, \n",
        "                     validation_data=(x_test, y_test),\n",
        "                     callbacks=callbacks_list)\n",
        "\n",
        "#evaluate toward test dataset\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss: ', score[0])\n",
        "print('Test accuracy: ', score[1])\n",
        "\n",
        "model.save('Test_FASHION_MNIST_MANO.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_26 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "batch_normalization_14 (Batc (None, 26, 26, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 24, 24, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 8, 8, 32)          51232     \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 8, 8, 32)          128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling (None, 4, 4, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 2, 2, 64)          18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 2, 2, 64)          256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_18 (MaxPooling (None, 1, 1, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 128)               8320      \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 98,922\n",
            "Trainable params: 98,538\n",
            "Non-trainable params: 384\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "60000/60000 [==============================] - 14s 232us/step - loss: 0.4183 - acc: 0.8463 - val_loss: 0.3705 - val_acc: 0.8685\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.86850, saving model to weights-improvement-01-0.87.hdf5\n",
            "Epoch 2/50\n",
            "60000/60000 [==============================] - 12s 206us/step - loss: 0.2683 - acc: 0.9025 - val_loss: 0.3515 - val_acc: 0.8731\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.86850 to 0.87310, saving model to weights-improvement-02-0.87.hdf5\n",
            "Epoch 3/50\n",
            "60000/60000 [==============================] - 12s 207us/step - loss: 0.2233 - acc: 0.9187 - val_loss: 0.2874 - val_acc: 0.8994\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.87310 to 0.89940, saving model to weights-improvement-03-0.90.hdf5\n",
            "Epoch 4/50\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 0.1944 - acc: 0.9289 - val_loss: 0.2568 - val_acc: 0.9084\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.89940 to 0.90840, saving model to weights-improvement-04-0.91.hdf5\n",
            "Epoch 5/50\n",
            "60000/60000 [==============================] - 12s 206us/step - loss: 0.1698 - acc: 0.9378 - val_loss: 0.2615 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.90840 to 0.90910, saving model to weights-improvement-05-0.91.hdf5\n",
            "Epoch 6/50\n",
            "60000/60000 [==============================] - 12s 205us/step - loss: 0.1501 - acc: 0.9448 - val_loss: 0.2567 - val_acc: 0.9124\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.90910 to 0.91240, saving model to weights-improvement-06-0.91.hdf5\n",
            "Epoch 7/50\n",
            "60000/60000 [==============================] - 12s 206us/step - loss: 0.1358 - acc: 0.9496 - val_loss: 0.2994 - val_acc: 0.9020\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.91240\n",
            "Epoch 8/50\n",
            "60000/60000 [==============================] - 12s 207us/step - loss: 0.1187 - acc: 0.9558 - val_loss: 0.2800 - val_acc: 0.9137\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.91240 to 0.91370, saving model to weights-improvement-08-0.91.hdf5\n",
            "Epoch 9/50\n",
            "60000/60000 [==============================] - 12s 206us/step - loss: 0.1055 - acc: 0.9616 - val_loss: 0.2916 - val_acc: 0.9130\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.91370\n",
            "Epoch 10/50\n",
            "60000/60000 [==============================] - 12s 206us/step - loss: 0.0968 - acc: 0.9636 - val_loss: 0.3228 - val_acc: 0.9070\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.91370\n",
            "Epoch 11/50\n",
            "60000/60000 [==============================] - 12s 206us/step - loss: 0.0844 - acc: 0.9687 - val_loss: 0.3217 - val_acc: 0.9102\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.91370\n",
            "Epoch 12/50\n",
            "60000/60000 [==============================] - 12s 206us/step - loss: 0.0761 - acc: 0.9720 - val_loss: 0.3541 - val_acc: 0.9097\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.91370\n",
            "Epoch 13/50\n",
            "60000/60000 [==============================] - 12s 206us/step - loss: 0.0692 - acc: 0.9736 - val_loss: 0.3480 - val_acc: 0.9127\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.91370\n",
            "Epoch 14/50\n",
            "60000/60000 [==============================] - 12s 206us/step - loss: 0.0620 - acc: 0.9768 - val_loss: 0.3781 - val_acc: 0.9079\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.91370\n",
            "Epoch 15/50\n",
            "60000/60000 [==============================] - 12s 207us/step - loss: 0.0574 - acc: 0.9784 - val_loss: 0.4116 - val_acc: 0.9078\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.91370\n",
            "Epoch 16/50\n",
            "60000/60000 [==============================] - 12s 204us/step - loss: 0.0550 - acc: 0.9800 - val_loss: 0.3909 - val_acc: 0.9089\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.91370\n",
            "Epoch 17/50\n",
            "60000/60000 [==============================] - 12s 205us/step - loss: 0.0491 - acc: 0.9813 - val_loss: 0.4001 - val_acc: 0.9104\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.91370\n",
            "Epoch 18/50\n",
            "60000/60000 [==============================] - 12s 205us/step - loss: 0.0447 - acc: 0.9832 - val_loss: 0.4412 - val_acc: 0.9082\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.91370\n",
            "Epoch 19/50\n",
            "60000/60000 [==============================] - 12s 207us/step - loss: 0.0424 - acc: 0.9846 - val_loss: 0.4356 - val_acc: 0.9091\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.91370\n",
            "Epoch 20/50\n",
            "60000/60000 [==============================] - 12s 204us/step - loss: 0.0391 - acc: 0.9851 - val_loss: 0.4360 - val_acc: 0.9081\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.91370\n",
            "Epoch 21/50\n",
            "60000/60000 [==============================] - 12s 208us/step - loss: 0.0380 - acc: 0.9865 - val_loss: 0.4447 - val_acc: 0.9094\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.91370\n",
            "Epoch 22/50\n",
            "60000/60000 [==============================] - 12s 207us/step - loss: 0.0359 - acc: 0.9869 - val_loss: 0.4630 - val_acc: 0.9149\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.91370 to 0.91490, saving model to weights-improvement-22-0.91.hdf5\n",
            "Epoch 23/50\n",
            "60000/60000 [==============================] - 12s 208us/step - loss: 0.0351 - acc: 0.9872 - val_loss: 0.4797 - val_acc: 0.9111\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.91490\n",
            "Epoch 24/50\n",
            "60000/60000 [==============================] - 12s 205us/step - loss: 0.0313 - acc: 0.9883 - val_loss: 0.5030 - val_acc: 0.9082\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.91490\n",
            "Epoch 25/50\n",
            "60000/60000 [==============================] - 12s 208us/step - loss: 0.0319 - acc: 0.9888 - val_loss: 0.4705 - val_acc: 0.9110\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.91490\n",
            "Epoch 26/50\n",
            "60000/60000 [==============================] - 12s 207us/step - loss: 0.0280 - acc: 0.9899 - val_loss: 0.5583 - val_acc: 0.9106\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.91490\n",
            "Epoch 27/50\n",
            "60000/60000 [==============================] - 12s 207us/step - loss: 0.0286 - acc: 0.9895 - val_loss: 0.5515 - val_acc: 0.9080\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.91490\n",
            "Epoch 28/50\n",
            "60000/60000 [==============================] - 12s 207us/step - loss: 0.0284 - acc: 0.9898 - val_loss: 0.5280 - val_acc: 0.9111\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.91490\n",
            "Epoch 29/50\n",
            "60000/60000 [==============================] - 12s 207us/step - loss: 0.0260 - acc: 0.9909 - val_loss: 0.5539 - val_acc: 0.9082\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.91490\n",
            "Epoch 30/50\n",
            "60000/60000 [==============================] - 12s 207us/step - loss: 0.0241 - acc: 0.9910 - val_loss: 0.5535 - val_acc: 0.9090\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.91490\n",
            "Epoch 31/50\n",
            "60000/60000 [==============================] - 12s 205us/step - loss: 0.0248 - acc: 0.9909 - val_loss: 0.5837 - val_acc: 0.9104\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.91490\n",
            "Epoch 32/50\n",
            "60000/60000 [==============================] - 12s 205us/step - loss: 0.0224 - acc: 0.9919 - val_loss: 0.5737 - val_acc: 0.9125\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.91490\n",
            "Epoch 33/50\n",
            "60000/60000 [==============================] - 12s 202us/step - loss: 0.0242 - acc: 0.9914 - val_loss: 0.6141 - val_acc: 0.9090\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.91490\n",
            "Epoch 34/50\n",
            "60000/60000 [==============================] - 12s 203us/step - loss: 0.0237 - acc: 0.9915 - val_loss: 0.6099 - val_acc: 0.9112\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.91490\n",
            "Epoch 35/50\n",
            "60000/60000 [==============================] - 12s 205us/step - loss: 0.0215 - acc: 0.9921 - val_loss: 0.5674 - val_acc: 0.9124\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.91490\n",
            "Epoch 36/50\n",
            "60000/60000 [==============================] - 12s 206us/step - loss: 0.0210 - acc: 0.9927 - val_loss: 0.6005 - val_acc: 0.9108\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.91490\n",
            "Epoch 37/50\n",
            "60000/60000 [==============================] - 12s 207us/step - loss: 0.0199 - acc: 0.9930 - val_loss: 0.5876 - val_acc: 0.9173\n",
            "\n",
            "Epoch 00037: val_acc improved from 0.91490 to 0.91730, saving model to weights-improvement-37-0.92.hdf5\n",
            "Epoch 38/50\n",
            "60000/60000 [==============================] - 12s 206us/step - loss: 0.0193 - acc: 0.9932 - val_loss: 0.6179 - val_acc: 0.9118\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.91730\n",
            "Epoch 39/50\n",
            "60000/60000 [==============================] - 12s 207us/step - loss: 0.0208 - acc: 0.9928 - val_loss: 0.6022 - val_acc: 0.9106\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.91730\n",
            "Epoch 40/50\n",
            "60000/60000 [==============================] - 12s 207us/step - loss: 0.0185 - acc: 0.9938 - val_loss: 0.6298 - val_acc: 0.9109\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.91730\n",
            "Epoch 41/50\n",
            "60000/60000 [==============================] - 12s 205us/step - loss: 0.0202 - acc: 0.9926 - val_loss: 0.6377 - val_acc: 0.9105\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.91730\n",
            "Epoch 42/50\n",
            "60000/60000 [==============================] - 13s 211us/step - loss: 0.0187 - acc: 0.9936 - val_loss: 0.6337 - val_acc: 0.9137\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.91730\n",
            "Epoch 43/50\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 0.0165 - acc: 0.9943 - val_loss: 0.6470 - val_acc: 0.9144\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.91730\n",
            "Epoch 44/50\n",
            "60000/60000 [==============================] - 12s 206us/step - loss: 0.0188 - acc: 0.9938 - val_loss: 0.5937 - val_acc: 0.9123\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.91730\n",
            "Epoch 45/50\n",
            "60000/60000 [==============================] - 12s 207us/step - loss: 0.0164 - acc: 0.9942 - val_loss: 0.6132 - val_acc: 0.9150\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.91730\n",
            "Epoch 46/50\n",
            "60000/60000 [==============================] - 12s 206us/step - loss: 0.0167 - acc: 0.9944 - val_loss: 0.6031 - val_acc: 0.9158\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.91730\n",
            "Epoch 47/50\n",
            "60000/60000 [==============================] - 12s 206us/step - loss: 0.0160 - acc: 0.9944 - val_loss: 0.6407 - val_acc: 0.9153\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.91730\n",
            "Epoch 48/50\n",
            "60000/60000 [==============================] - 12s 205us/step - loss: 0.0168 - acc: 0.9944 - val_loss: 0.6302 - val_acc: 0.9174\n",
            "\n",
            "Epoch 00048: val_acc improved from 0.91730 to 0.91740, saving model to weights-improvement-48-0.92.hdf5\n",
            "Epoch 49/50\n",
            "60000/60000 [==============================] - 12s 206us/step - loss: 0.0160 - acc: 0.9944 - val_loss: 0.6035 - val_acc: 0.9146\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.91740\n",
            "Epoch 50/50\n",
            "60000/60000 [==============================] - 12s 207us/step - loss: 0.0143 - acc: 0.9951 - val_loss: 0.6106 - val_acc: 0.9144\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.91740\n",
            "Test loss:  0.6105943850596436\n",
            "Test accuracy:  0.9144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-w_J_fOD7wd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://gist.github.com/fchollet/0830affa1f7f19fd47b06d4cf89ed44d\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1. / 255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "train_generator = train_datagen.flow(\n",
        "    x_train),\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow(\n",
        "    x-test),\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=nb_train_samples // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=nb_validation_samples // batch_size)\n",
        "\n",
        "model.save_weights('first_try.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qi8a128oEBn6",
        "colab_type": "text"
      },
      "source": [
        "ajouter ça pour la data augmentation!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwmlm7iBht2B",
        "colab_type": "code",
        "outputId": "c17bdf83-4f67-4778-f351-121ba6ef5d04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#model from : https://www.dlology.com/blog/one-simple-trick-to-train-keras-model-faster-with-batch-normalization/\n",
        "#Epoch 00050: val_acc did not improve from 0.90950\n",
        "#Test loss:  0.7327184855340101\n",
        "#Test accuracy:  0.902\n",
        "\n",
        "#En rajoutant Dense 128\n",
        "#Epoch 00050: val_acc did not improve from 0.91210\n",
        "#Test loss:  0.7428650218252558\n",
        "#Test accuracy:  0.9072\n",
        "\n",
        "#En rajoutant Dense 256\n",
        "#Total params: 246,008\n",
        "#Epoch 00050: val_acc did not improve from 0.91430\n",
        "#Test loss:  0.6275242215685546\n",
        "#Test accuracy:  0.9134\n",
        "\n",
        "#Dernier résultat :\n",
        "#Epoch 00050: val_acc did not improve from 0.91700\n",
        "#Test loss:  0.7110278423965914\n",
        "#Test accuracy:  0.9069\n",
        "  \n",
        "from __future__ import print_function\n",
        "\n",
        "import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten,Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.optimizers import RMSprop\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 50\n",
        "\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "################## Def dataset ###########################\n",
        "\n",
        "#the data, split between train and test sets\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "#x_train = x_train.reshape(60000, 56)\n",
        "#x_test = x_test.reshape(10000, 56)\n",
        "\n",
        "if K.image_data_format() == 'channels first':\n",
        "  x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "  x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "  input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "  x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "  x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "  input_shape = (img_rows, img_cols, 1)\n",
        "  \n",
        "  \n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "#to ensure a good dynamic, help the learnig convergence\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "#convert class vectors to binary class matrices\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "################## Data augmentation ###################\n",
        "\n",
        "################## Def model ###########################\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, kernel_size=(3,3),\n",
        "                activation='relu',\n",
        "                input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(Conv2D(64, (3,3), activation='relu')) # meilleur sans les deux couches suivantes\n",
        "model.add(BatchNormalization())\n",
        "model.add(Flatten())\n",
        "\n",
        "\n",
        "#Epoch 00050: val_acc did not improve from 0.91760\n",
        "#Test loss:  0.6874618720195256\n",
        "#Test accuracy:  0.9066\n",
        "\n",
        "#Avec batchNormalization\n",
        "#Epoch 00050: val_acc did not improve from 0.91740\n",
        "#Test loss:  0.6105943850596436\n",
        "#Test accuracy:  0.9144\n",
        "\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "#model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "#            optimizer=keras.optimizers.Adadelta(),\n",
        "#             metrics=['accuracy'])\n",
        "#Epoch 00050: val_acc did not improve from 0.91970\n",
        "#Test loss:  0.6960430979938712\n",
        "#Test accuracy:  0.917\n",
        "  \n",
        "  \n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer = RMSprop(), metrics=['accuracy'])\n",
        "##Epoch 00050: val_acc did not improve from 0.92170\n",
        "##Test loss:  0.7203312129669707\n",
        "##Test accuracy:  0.9142\n",
        "\n",
        "################## Learning ###########################\n",
        "\n",
        "#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
        "\n",
        "\n",
        "# checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "#fit permet de lancer l'apprentissage, avec les donnees, les labels, la taille du batch, le nb d'epoque, la verbosite, et les donnees de validation d'epoque a epoque\n",
        "history = model.fit(x_train, y_train, \n",
        "                     batch_size=batch_size, \n",
        "                     epochs=epochs, \n",
        "                     verbose=1, \n",
        "                     validation_data=(x_test, y_test),\n",
        "                     callbacks=callbacks_list)\n",
        "\n",
        "#evaluate toward test dataset\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss: ', score[0])\n",
        "print('Test accuracy: ', score[1])\n",
        "\n",
        "model.save('Test_FASHION_MNIST_MANO.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_39 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 26, 26, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_25 (MaxPooling (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_40 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_28 (Batc (None, 11, 11, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_26 (MaxPooling (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_41 (Conv2D)           (None, 3, 3, 64)          36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_29 (Batc (None, 3, 3, 64)          256       \n",
            "_________________________________________________________________\n",
            "flatten_11 (Flatten)         (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 512)               295424    \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 10)                650       \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 10)                110       \n",
            "=================================================================\n",
            "Total params: 525,048\n",
            "Trainable params: 524,728\n",
            "Non-trainable params: 320\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "60000/60000 [==============================] - 11s 176us/step - loss: 0.5709 - acc: 0.8012 - val_loss: 0.5557 - val_acc: 0.8319\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.83190, saving model to weights-improvement-01-0.83.hdf5\n",
            "Epoch 2/50\n",
            "60000/60000 [==============================] - 9s 144us/step - loss: 0.3023 - acc: 0.8922 - val_loss: 0.3534 - val_acc: 0.8754\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.83190 to 0.87540, saving model to weights-improvement-02-0.88.hdf5\n",
            "Epoch 3/50\n",
            "60000/60000 [==============================] - 8s 137us/step - loss: 0.2538 - acc: 0.9089 - val_loss: 0.3034 - val_acc: 0.8913\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.87540 to 0.89130, saving model to weights-improvement-03-0.89.hdf5\n",
            "Epoch 4/50\n",
            "60000/60000 [==============================] - 8s 135us/step - loss: 0.2215 - acc: 0.9200 - val_loss: 0.3405 - val_acc: 0.8866\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.89130\n",
            "Epoch 5/50\n",
            "60000/60000 [==============================] - 8s 133us/step - loss: 0.1985 - acc: 0.9283 - val_loss: 0.3835 - val_acc: 0.8761\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.89130\n",
            "Epoch 6/50\n",
            "60000/60000 [==============================] - 8s 133us/step - loss: 0.1822 - acc: 0.9346 - val_loss: 0.2900 - val_acc: 0.9068\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.89130 to 0.90680, saving model to weights-improvement-06-0.91.hdf5\n",
            "Epoch 7/50\n",
            "60000/60000 [==============================] - 8s 133us/step - loss: 0.1654 - acc: 0.9407 - val_loss: 0.4875 - val_acc: 0.8620\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.90680\n",
            "Epoch 8/50\n",
            "60000/60000 [==============================] - 8s 132us/step - loss: 0.1501 - acc: 0.9459 - val_loss: 0.3139 - val_acc: 0.9137\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.90680 to 0.91370, saving model to weights-improvement-08-0.91.hdf5\n",
            "Epoch 9/50\n",
            "60000/60000 [==============================] - 8s 132us/step - loss: 0.1405 - acc: 0.9508 - val_loss: 0.3666 - val_acc: 0.8889\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.91370\n",
            "Epoch 10/50\n",
            "60000/60000 [==============================] - 8s 133us/step - loss: 0.1331 - acc: 0.9536 - val_loss: 0.3398 - val_acc: 0.9106\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.91370\n",
            "Epoch 11/50\n",
            "60000/60000 [==============================] - 8s 133us/step - loss: 0.1219 - acc: 0.9577 - val_loss: 0.3852 - val_acc: 0.9063\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.91370\n",
            "Epoch 12/50\n",
            "60000/60000 [==============================] - 8s 132us/step - loss: 0.1181 - acc: 0.9594 - val_loss: 0.3932 - val_acc: 0.9075\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.91370\n",
            "Epoch 13/50\n",
            "60000/60000 [==============================] - 8s 136us/step - loss: 0.1112 - acc: 0.9618 - val_loss: 0.3686 - val_acc: 0.9069\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.91370\n",
            "Epoch 14/50\n",
            "60000/60000 [==============================] - 8s 134us/step - loss: 0.1036 - acc: 0.9639 - val_loss: 0.4349 - val_acc: 0.9087\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.91370\n",
            "Epoch 15/50\n",
            "60000/60000 [==============================] - 8s 132us/step - loss: 0.0990 - acc: 0.9664 - val_loss: 0.4366 - val_acc: 0.9107\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.91370\n",
            "Epoch 16/50\n",
            "60000/60000 [==============================] - 8s 132us/step - loss: 0.0986 - acc: 0.9671 - val_loss: 0.4602 - val_acc: 0.9047\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.91370\n",
            "Epoch 17/50\n",
            "60000/60000 [==============================] - 8s 132us/step - loss: 0.0932 - acc: 0.9701 - val_loss: 0.4807 - val_acc: 0.9170\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.91370 to 0.91700, saving model to weights-improvement-17-0.92.hdf5\n",
            "Epoch 18/50\n",
            "60000/60000 [==============================] - 8s 133us/step - loss: 0.0948 - acc: 0.9697 - val_loss: 0.5354 - val_acc: 0.9034\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.91700\n",
            "Epoch 19/50\n",
            "60000/60000 [==============================] - 8s 133us/step - loss: 0.0877 - acc: 0.9725 - val_loss: 0.5412 - val_acc: 0.9087\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.91700\n",
            "Epoch 20/50\n",
            "60000/60000 [==============================] - 8s 133us/step - loss: 0.0866 - acc: 0.9724 - val_loss: 0.4800 - val_acc: 0.9140\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.91700\n",
            "Epoch 21/50\n",
            "60000/60000 [==============================] - 8s 133us/step - loss: 0.0846 - acc: 0.9747 - val_loss: 0.5045 - val_acc: 0.9098\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.91700\n",
            "Epoch 22/50\n",
            "60000/60000 [==============================] - 8s 133us/step - loss: 0.0813 - acc: 0.9756 - val_loss: 0.4307 - val_acc: 0.9115\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.91700\n",
            "Epoch 23/50\n",
            "60000/60000 [==============================] - 8s 137us/step - loss: 0.0795 - acc: 0.9762 - val_loss: 0.5138 - val_acc: 0.9086\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.91700\n",
            "Epoch 24/50\n",
            "60000/60000 [==============================] - 8s 134us/step - loss: 0.0768 - acc: 0.9777 - val_loss: 0.7091 - val_acc: 0.8827\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.91700\n",
            "Epoch 25/50\n",
            "60000/60000 [==============================] - 8s 133us/step - loss: 0.0787 - acc: 0.9765 - val_loss: 0.5311 - val_acc: 0.9060\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.91700\n",
            "Epoch 26/50\n",
            "60000/60000 [==============================] - 8s 133us/step - loss: 0.0806 - acc: 0.9767 - val_loss: 0.4796 - val_acc: 0.9112\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.91700\n",
            "Epoch 27/50\n",
            "60000/60000 [==============================] - 8s 132us/step - loss: 0.0814 - acc: 0.9781 - val_loss: 0.7270 - val_acc: 0.8918\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.91700\n",
            "Epoch 28/50\n",
            "60000/60000 [==============================] - 8s 133us/step - loss: 0.0737 - acc: 0.9787 - val_loss: 0.5843 - val_acc: 0.9017\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.91700\n",
            "Epoch 29/50\n",
            "60000/60000 [==============================] - 8s 132us/step - loss: 0.0759 - acc: 0.9795 - val_loss: 0.5453 - val_acc: 0.8997\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.91700\n",
            "Epoch 30/50\n",
            "60000/60000 [==============================] - 8s 133us/step - loss: 0.0754 - acc: 0.9789 - val_loss: 1.0594 - val_acc: 0.8787\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.91700\n",
            "Epoch 31/50\n",
            "60000/60000 [==============================] - 8s 133us/step - loss: 0.0738 - acc: 0.9801 - val_loss: 0.6326 - val_acc: 0.9086\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.91700\n",
            "Epoch 32/50\n",
            "60000/60000 [==============================] - 8s 135us/step - loss: 0.0746 - acc: 0.9799 - val_loss: 0.5850 - val_acc: 0.9057\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.91700\n",
            "Epoch 33/50\n",
            "60000/60000 [==============================] - 8s 134us/step - loss: 0.0741 - acc: 0.9804 - val_loss: 0.5863 - val_acc: 0.9046\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.91700\n",
            "Epoch 34/50\n",
            "60000/60000 [==============================] - 8s 134us/step - loss: 0.0721 - acc: 0.9810 - val_loss: 0.6246 - val_acc: 0.9015\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.91700\n",
            "Epoch 35/50\n",
            "60000/60000 [==============================] - 8s 132us/step - loss: 0.0724 - acc: 0.9815 - val_loss: 0.6645 - val_acc: 0.9080\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.91700\n",
            "Epoch 36/50\n",
            "60000/60000 [==============================] - 8s 134us/step - loss: 0.0764 - acc: 0.9811 - val_loss: 0.6689 - val_acc: 0.9081\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.91700\n",
            "Epoch 37/50\n",
            "60000/60000 [==============================] - 8s 133us/step - loss: 0.0711 - acc: 0.9826 - val_loss: 0.6494 - val_acc: 0.8973\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.91700\n",
            "Epoch 38/50\n",
            "60000/60000 [==============================] - 8s 132us/step - loss: 0.0701 - acc: 0.9829 - val_loss: 0.6922 - val_acc: 0.9057\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.91700\n",
            "Epoch 39/50\n",
            "60000/60000 [==============================] - 8s 134us/step - loss: 0.0739 - acc: 0.9820 - val_loss: 0.6391 - val_acc: 0.9067\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.91700\n",
            "Epoch 40/50\n",
            "60000/60000 [==============================] - 8s 133us/step - loss: 0.0762 - acc: 0.9824 - val_loss: 0.6767 - val_acc: 0.9124\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.91700\n",
            "Epoch 41/50\n",
            "60000/60000 [==============================] - 8s 133us/step - loss: 0.0711 - acc: 0.9832 - val_loss: 0.6459 - val_acc: 0.9041\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.91700\n",
            "Epoch 42/50\n",
            "60000/60000 [==============================] - 8s 133us/step - loss: 0.0734 - acc: 0.9837 - val_loss: 0.6104 - val_acc: 0.9112\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.91700\n",
            "Epoch 43/50\n",
            "60000/60000 [==============================] - 8s 133us/step - loss: 0.0806 - acc: 0.9823 - val_loss: 0.5760 - val_acc: 0.9112\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.91700\n",
            "Epoch 44/50\n",
            "60000/60000 [==============================] - 8s 132us/step - loss: 0.0727 - acc: 0.9828 - val_loss: 0.8440 - val_acc: 0.8990\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.91700\n",
            "Epoch 45/50\n",
            "60000/60000 [==============================] - 8s 133us/step - loss: 0.0744 - acc: 0.9832 - val_loss: 0.6752 - val_acc: 0.8963\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.91700\n",
            "Epoch 46/50\n",
            "60000/60000 [==============================] - 8s 133us/step - loss: 0.0732 - acc: 0.9838 - val_loss: 0.7552 - val_acc: 0.9113\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.91700\n",
            "Epoch 47/50\n",
            "60000/60000 [==============================] - 8s 133us/step - loss: 0.0838 - acc: 0.9818 - val_loss: 0.6247 - val_acc: 0.9103\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.91700\n",
            "Epoch 48/50\n",
            "60000/60000 [==============================] - 8s 133us/step - loss: 0.0762 - acc: 0.9835 - val_loss: 0.7072 - val_acc: 0.9053\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.91700\n",
            "Epoch 49/50\n",
            "60000/60000 [==============================] - 8s 133us/step - loss: 0.0725 - acc: 0.9849 - val_loss: 0.6342 - val_acc: 0.9001\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.91700\n",
            "Epoch 50/50\n",
            "60000/60000 [==============================] - 8s 132us/step - loss: 0.0703 - acc: 0.9837 - val_loss: 0.7110 - val_acc: 0.9069\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.91700\n",
            "Test loss:  0.7110278423965914\n",
            "Test accuracy:  0.9069\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6Cw0p8qq2pt",
        "colab_type": "text"
      },
      "source": [
        "Comparer les deux solutions ci dessus et en choisir une.\n",
        "Ajouter de la dataAugmentation dedans."
      ]
    }
  ]
}