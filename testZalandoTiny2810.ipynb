{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "testZalandoTiny2810.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwmlm7iBht2B",
        "colab_type": "code",
        "outputId": "68b04149-3cc9-45d9-8706-6b9c7105bc0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Inspiration du model Tiny Darknet https://pjreddie.com/darknet/tiny-darknet/\n",
        "#https://github.com/pjreddie/darknet/blob/master/cfg/tiny.cfg\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, SpatialDropout2D, GlobalAveragePooling2D, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.optimizers import RMSprop\n",
        "from keras import backend as K\n",
        "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 100\n",
        "\n",
        "img_rows, img_cols = 28, 28\n",
        "nb_train_samples = 60000\n",
        "nb_validation_samples = 10000\n",
        "\n",
        "################## Def dataset ###########################\n",
        "\n",
        "#the data, split between train and test sets\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "#x_train = x_train.reshape(60000, 56)\n",
        "#x_test = x_test.reshape(10000, 56)\n",
        "\n",
        "if K.image_data_format() == 'channels first':\n",
        "  x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "  x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "  input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "  x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "  x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "  input_shape = (img_rows, img_cols, 1)\n",
        "  \n",
        "  \n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "#to ensure a good dynamic, help the learnig convergence\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "#convert class vectors to binary class matrices\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "################## Data augmentation ###################\n",
        "\n",
        "train_datagen = ImageDataGenerator(horizontal_flip=True)\n",
        "\n",
        "train_generator = train_datagen.flow(\n",
        "  x_train,\n",
        "  y_train,\n",
        "  batch_size=batch_size)\n",
        "\n",
        "################## Def model ###########################\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(1,1),\n",
        "                activation='relu',\n",
        "                input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(256, kernel_size=(3,3),\n",
        "                activation='relu',\n",
        "                input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32, kernel_size=(1,1),\n",
        "                activation='relu',\n",
        "                input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(256, kernel_size=(3,3),\n",
        "                activation='relu',\n",
        "                input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(SpatialDropout2D(0.3))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "\n",
        "model.add(Conv2D(64, kernel_size=(1,1),\n",
        "                activation='relu',\n",
        "                input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(512, kernel_size=(3,3),\n",
        "                activation='relu',\n",
        "                input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, kernel_size=(1,1),\n",
        "                activation='relu',\n",
        "                input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(512, kernel_size=(3,3),\n",
        "                activation='relu',\n",
        "                input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, kernel_size=(1,1),\n",
        "                activation='relu',\n",
        "                input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(1000, kernel_size=(1,1),\n",
        "                activation='relu',\n",
        "                input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(GlobalAveragePooling2D())\n",
        "\n",
        "\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "#model.add(BatchNormalization())\n",
        "  \n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer = RMSprop(), metrics=['accuracy'])\n",
        "\n",
        "\n",
        "################## Learning ###########################\n",
        "\n",
        "#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
        "\n",
        "\n",
        "# checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\"\"\"\n",
        "earlystopping = EarlyStopping(monitor='val_acc',\n",
        "                              min_delta=0,\n",
        "                              patience=5,\n",
        "                              verbose=1, mode='auto')\n",
        "callbacks_list = [checkpoint, earlystopping]\n",
        "\"\"\"\n",
        "callbacks_list = [checkpoint]\n",
        "\n",
        "#fit permet de lancer l'apprentissage, avec les donnees, les labels, la taille du batch, le nb d'epoque, la verbosite, et les donnees de validation d'epoque a epoque\n",
        "\"\"\"\n",
        "history = model.fit(x_train, y_train, \n",
        "                     batch_size=batch_size, \n",
        "                     epochs=epochs, \n",
        "                     verbose=1, \n",
        "                     validation_data=(x_test, y_test),\n",
        "                     callbacks=callbacks_list)\n",
        "\"\"\"\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=nb_train_samples // batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=callbacks_list)\n",
        "\n",
        "#evaluate toward test dataset\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss: ', score[0])\n",
        "print('Test accuracy: ', score[1])\n",
        "\n",
        "model.save_weights('fashion_try_0110.h5')\n",
        "model.save('Test_FASHION_MNIST_V1.h5')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "60000 train samples\n",
            "10000 test samples\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_26 (Conv2D)           (None, 28, 28, 32)        64        \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 28, 28, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 26, 26, 256)       73984     \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 26, 26, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 26, 26, 32)        8224      \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 26, 26, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 24, 24, 256)       73984     \n",
            "_________________________________________________________________\n",
            "batch_normalization_28 (Batc (None, 24, 24, 256)       1024      \n",
            "_________________________________________________________________\n",
            "spatial_dropout2d_4 (Spatial (None, 24, 24, 256)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_30 (Conv2D)           (None, 12, 12, 64)        16448     \n",
            "_________________________________________________________________\n",
            "batch_normalization_29 (Batc (None, 12, 12, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_31 (Conv2D)           (None, 10, 10, 512)       295424    \n",
            "_________________________________________________________________\n",
            "batch_normalization_30 (Batc (None, 10, 10, 512)       2048      \n",
            "_________________________________________________________________\n",
            "conv2d_32 (Conv2D)           (None, 10, 10, 64)        32832     \n",
            "_________________________________________________________________\n",
            "batch_normalization_31 (Batc (None, 10, 10, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_33 (Conv2D)           (None, 8, 8, 512)         295424    \n",
            "_________________________________________________________________\n",
            "batch_normalization_32 (Batc (None, 8, 8, 512)         2048      \n",
            "_________________________________________________________________\n",
            "conv2d_34 (Conv2D)           (None, 8, 8, 128)         65664     \n",
            "_________________________________________________________________\n",
            "batch_normalization_33 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_35 (Conv2D)           (None, 8, 8, 1000)        129000    \n",
            "_________________________________________________________________\n",
            "batch_normalization_34 (Batc (None, 8, 8, 1000)        4000      \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_3 ( (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                10010     \n",
            "=================================================================\n",
            "Total params: 1,012,482\n",
            "Trainable params: 1,006,770\n",
            "Non-trainable params: 5,712\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "468/468 [==============================] - 79s 170ms/step - loss: 0.5391 - acc: 0.8034 - val_loss: 0.3729 - val_acc: 0.8765\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.87650, saving model to weights-improvement-01-0.88.hdf5\n",
            "Epoch 2/100\n",
            "289/468 [=================>............] - ETA: 27s - loss: 0.3303 - acc: 0.8816"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-fa05e86ba7b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     callbacks=callbacks_list)\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;31m#evaluate toward test dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    214\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOf5v2k_w5Qk",
        "colab_type": "text"
      },
      "source": [
        "Voir si je rajoute des couches Flatten"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx6sDiAoWWcg",
        "colab_type": "text"
      },
      "source": [
        "model from : https://www.dlology.com/blog/one-simple-trick-to-train-keras-model-faster-with-batch-normalization/\n",
        "\n",
        "Epoch 00050: val_acc did not improve from 0.90950\n",
        "\n",
        "Test loss:  0.7327184855340101\n",
        "\n",
        "Test accuracy:  0.902\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#En rajoutant Dense 128\n",
        "\n",
        "Epoch 00050: val_acc did not improve from 0.91210# \n",
        "\n",
        "Test loss:  0.7428650218252558\n",
        "\n",
        "Test accuracy:  0.9072\n",
        "\n",
        "\n",
        "\n",
        "#En rajoutant Dense 256\n",
        "\n",
        "Total params: 246,008\n",
        "\n",
        "Epoch 00050: val_acc did not improvefrom 0.91430\n",
        "\n",
        "Test loss:  0.6275242215685546\n",
        "\n",
        "Test accuracy:  0.9134\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Dernier résultat :\n",
        "\n",
        "Epoch 00050: val_acc did not improve from 0.91700\n",
        "\n",
        "Test loss:  0.7110278423965914\n",
        "\n",
        "Test accuracy:  0.9069"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6Cw0p8qq2pt",
        "colab_type": "text"
      },
      "source": [
        "#Data Augmentation\n",
        "En ajoutant de la data augmentation via un horizontal_flip : \n",
        "\n",
        "Total params : 525,048\n",
        "\n",
        "Test loss:  0.3304\n",
        "\n",
        "Test accuracy:  0.9150"
      ]
    }
  ]
}